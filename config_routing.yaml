# MIDAS Routing Optimization Configuration
# This trains the routing module on a FROZEN baseline model for inference optimization

# Data configuration
data:
  # Use pre-split CICIDS2017 data
  train_path: "data/processed/train.csv"
  val_path: "data/processed/val.csv"
  test_path: "data/processed/test.csv"

  # Target column name in CSV
  target_column: "label"

  # Feature normalization
  normalize: true

  # Batch size
  batch_size: 512

  # Random seed
  random_seed: 42

# Model architecture (must match baseline)
model:
  # Network type: "mlp"
  type: "mlp"

  # Hidden layer dimensions - MUST MATCH BASELINE
  hidden_dims: [256, 512, 512, 256, 128]

  # Exit layer indices - MUST MATCH BASELINE
  exit_layers: [1, 3, 4]

  # Number of output classes
  num_classes: 2

  # Dropout rates
  dropout: 0.3
  exit_dropout: 0.2

# Routing module configuration
routing:
  # Routing type: "attention" is more powerful than "mlp"
  type: "attention"

  # Routing network hidden dimension
  hidden_dim: 128

  # For attention routing
  num_heads: 4
  num_layers: 2

  # Temperature for probability scaling (lower = more confident decisions)
  temperature: 1.0

  # Context dimension (for running statistics)
  context_dim: 16

# Cost model configuration
cost:
  # Cost type: "layer_depth"
  type: "layer_depth"

  # Layer-specific costs (computational cost at each layer)
  cost_per_layer: [1.0, 2.0, 3.0, 4.0, 5.0]

  # Cost penalty weight in reward function
  # Higher lambda = more aggressive cost reduction
  # Lower lambda = prioritize accuracy
  lambda: 0.15  # Balanced cost-accuracy tradeoff

  # Print a FLOPs profile derived from architecture (optional)
  print_flops_profile: true

# Training configuration
training:
  # IMPORTANT: Load baseline checkpoint
  load_checkpoint: "checkpoints/baseline_best.pt"

  # IMPORTANT: Freeze backbone network (train routing only)
  freeze_backbone: true

  # Number of training epochs (routing converges faster)
  epochs: 30

  # Learning rates
  lr_classifier: 0.0  # Frozen, not trained
  lr_routing: 0.0005  # Only routing is trained

  # Optimizer
  optimizer: "adamw"
  weight_decay: 0.0001

  # Learning rate schedule
  scheduler: "cosine"

  # Loss weights
  exit_loss_weights: [0.3, 0.5, 1.0]
  routing_loss_weight: 1.0  # Balance between classification and routing

  # RL training
  rl:
    # Advantage estimation (reduces variance)
    advantage_beta: 0.99

    # Entropy regularization (encourages exploration)
    entropy_coef: 0.02  # Slightly higher to explore exit strategies

    # NO warmup - train routing from start
    warmup_epochs: 0

  # Early stopping
  early_stopping:
    enabled: true
    patience: 8
    metric: "val_overall_accuracy"  # or "val_avg_cost" for cost-aware metric

  # Gradient clipping
  grad_clip: 1.0

  # Device
  device: "mps"  # Use Apple Silicon GPU

# Evaluation configuration
evaluation:
  # Oracle baselines to compare against
  oracle_baselines:
    - "always_exit_0"    # Always exit at first exit (fastest, likely less accurate)
    - "always_exit_1"    # Always exit at second exit
    - "always_final"     # Always use final exit (baseline accuracy, highest cost)
    - "random"           # Random exit selection
    - "confidence"       # Exit when confidence > threshold

  # Confidence threshold for confidence-based oracle
  confidence_threshold: 0.9

  # Metrics to compute
  metrics:
    - "accuracy"
    - "per_exit_accuracy"
    - "exit_usage"
    - "avg_cost"
    - "accuracy_per_cost"
    - "cost_accuracy_tradeoff"

  # Where to save evaluation outputs (CSV/JSON)
  output_dir: "results"

# Weights & Biases configuration
wandb:
  enabled: true  # Disable for initial testing
  project: "MIDAS-CICIDS2017"
  entity: null

  run_name: "routing_optimization"

  tags:
    - "routing"
    - "inference-optimization"
    - "cicids2017"
    - "binary"
    - "frozen-backbone"

  log_interval: 50

  save_checkpoints: true
  # Avoid overwriting your baseline: choose a distinct filename
  checkpoint_name: "routing_best.pt"
